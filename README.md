# Overview

Arachnid is both a general web spider and a reddit bot. It consumes links from subreddits specified in a config file (config.py), and goes N links deep on those pages (specified in the same config.py) looking for pdf files. When it finds a link to a pdf file it downloads it to its working directory.

# Architechture

Arachnid is divided into several constituent parts, namely:
- LinkManager, which provides a lazy stream of urls,
- TargetFilter, which is a predicate that checks if a url is of the target filetype,
- DownloadManager, which fetches the given file,
- and Arachnid itself, which continuously feeds urls from LinkManager into TargetFilter and then DownloadManager.

## LinkManager

LinkManager is the abstraction over the process of finding urls. LinkManager initializes the classes which produce the root links we will be crawling over (just reddit for the time being), and passes them to a LinkSource, which provides a single interface for getting links from each of the LinkProducers. It then passes the LinkSource it creates to a LinkExplorer, which crawls over urls generated by its LinkSource and yields the stream of urls it finds (including those from the LinkSource).

Since crawling over this many links is a time consuming process (often taking several minutes, even with a relatively low page depth such as 3), every class that LinkManager is composed of (and LinkManager itself) implements a get_links method, which must be a generator - meaning that links are calculated and yielded as they are asked for (i.e. lazily). This allows Arachnid to download target files as soon as they are found, and urls are able to be propagated through the function pipeline one at a time as they are found. Before these methods were generators, Arachnid would have to crawl first and only afterwards would it filter the urls for target files and download them. The implementation of generators has allowed Arachnid to perform much faster, and continuously crawl while downloading files. Eventually it is my plan to make the link crawling parallel, that way LinkExplorer doesn't have to wait for concurrent crawls to finish before starting on the next root url.

### LinkProducer

LinkProducer is an abstract interface for classes which produce urls. The idea behind this is to make it easy to add more link sources later (other than reddit) and for LinkSource to be able to treat every LinkProducer the same.

As mentioned before, a LinkProducer must implement the get_links method as a generator which yields urls. In addition, a LinkProducer must be initialized with a domain which is used to filter out urls that link back to a producer domain during crawling (this was causing Arachnid to crawl subreddits other than those provided in config.py).

#### RedditBot

At the moment, RedditBot is our only LinkProducer. It uses praw (a python wrapper around the reddit API) to fetch links from subreddits specified in config.py. Eventually I would like to extend this to parse links referenced in comments as well.

### LinkSource

LinkSource is an abstraction over a collection of LinkProducers - it simply iterates over each of its producers and yields links from their get_links method.

In addition, LinkSource provides an in_producer_domain method which returns whether a given url matches any of the domains of LinkSource's producers. This method is used to filter out crawled urls which map back to the domain of the producers.

### LinkExplorer

LinkExplorer is the class that does the crawling of the links provided by its LinkSource. LinkExplorer has an explore method, which lazily generates these crawled links one at a time. Explore crawls links by iterating over root urls provided by the source, fetching their contents, and parsing the contents for `<a href="foo.com">` tags and extracting the href component. It adds these to a set of urls, and recursively calls itself with each url in the set (decrementing a counter which is initialized as the page_depth provided by config.py).

## TargetFilter

TargetFilter is a module with a single function, target_filter, which takes a single url and returns whether or not it matches the target filetype provided in config.py. This function is mapped over the lazy stream of urls given by the LinkManager, and if it matches, the url is passed to the DownloadManager.

## DownloadManager

DownloadManager is a module with a single function, download_url, which takes a single url and writes the file to disk. It is called on a url after it passes the TargetFilter predicate.

## Arachnid

Arachnid creates an instance of LinkManager, and then iterates over urls given by the LinkManager ad infinitum - mapping TargetFilter over each url, and passing the urls which match to the DownloadManager.

# Notes

One style issue I know Arachnid has is that I have wrapped many calls in a try, except block without specifying an exception (which will handle any exception). I realize this is frowned upon, but for the time being the only goal of this program is to download as many files as possible, and I am receiving many different kinds of exceptions (Unicode errors, timeouts, 404 errors, etc.), which makes sense due to the sheer amount of urls Arachnid crawls. I plan on going through the logs and interpreting each different Exception I find by hand so that I can improve the robustness and reliability of Arachnid, but until I have time to do so Arachnid definitely does what I set out to make it do.

If you would like to contribute to the development of Arachnid, please fork it and submit a pull request. More LinkProducers would always be welcome.

If you experience any bugs during your use of Arachnid, please file an issue on this repository and I would be happy to look into it.
